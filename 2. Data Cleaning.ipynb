{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebd9d1b",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "---\n",
    "Take the raw data from `/data/hamSpam.csv` and `/data/phish.csv` and conform the ham/spam dataset to the structure of the phishing dataset and integrate together. Then clean the dataset by removing NA values, standardize the textual features by lowercasing and removing stopwords, and remove any embedded HTML elements from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d087e",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47a1155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff466ba0",
   "metadata": {},
   "source": [
    "## 2. Load Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caa72205",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamSpam_df = pd.read_csv(\"./data/hamSpam.csv\")\n",
    "phish_df = pd.read_csv(\"./data/phish.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6af1e",
   "metadata": {},
   "source": [
    "## 3. Process Ham/Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6717d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename known columns (no error if source names are missing)\n",
    "hamSpam_df.rename(\n",
    "    columns={\"Spam/Ham\": \"Email Type\", \"Message\": \"Email Text\"},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Drop optional columns only if they exist to avoid KeyError\n",
    "cols_to_drop = [c for c in [\"Date\", \"Subject\"] if c in hamSpam_df.columns]\n",
    "\n",
    "if cols_to_drop:\n",
    "\thamSpam_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Filter out emails that are not ham/spam\n",
    "hamSpam_df = hamSpam_df[hamSpam_df['Email Type'].isin(['ham', 'spam'])]\n",
    "\n",
    "# Export as processed dataset\n",
    "hamSpam_df.to_csv(\n",
    "    \"data/hamSpam_processed.csv\",\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    escapechar=\"\\\\\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbf923",
   "metadata": {},
   "source": [
    "## 4. Process Phishing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16281a1",
   "metadata": {},
   "source": [
    "Observe email types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "498aead1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'Safe Email'\", \"'Phishing Email'\"], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_df['Email Type'].apply(repr).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a9b71",
   "metadata": {},
   "source": [
    "Standardize email types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20061877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'ham'\", \"'phish'\"], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_df['Email Type'] = phish_df['Email Type'].replace(\n",
    "    {'Safe Email': 'ham', 'Phishing Email': 'phish'}\n",
    ")\n",
    "phish_df['Email Type'].apply(repr).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db54d59",
   "metadata": {},
   "source": [
    "## 5. Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a1953e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'ham'\", \"'spam'\", \"'phish'\"], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailDataset = pd.concat([hamSpam_df, phish_df], ignore_index=True)\n",
    "emailDataset['Email Type'].apply(repr).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db94af",
   "metadata": {},
   "source": [
    "Show total count of emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0b5c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1    52366\n",
       "Unnamed: 0      52366\n",
       "Email Text      52298\n",
       "Email Type      52366\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailDataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308d54c",
   "metadata": {},
   "source": [
    "## 6. Drop NA and \"empty\" Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c190657e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gary , production from the high island larger ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>- calpine daily gas nomination 1 . doc</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>jackie ,\\nsince the inlet to 3 river plant is ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52360</th>\n",
       "      <td>18644</td>\n",
       "      <td>18645</td>\n",
       "      <td>\\nRick Moen  a Ã©crit:&gt; &gt; I'm confused. I thou...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52361</th>\n",
       "      <td>18645</td>\n",
       "      <td>18646</td>\n",
       "      <td>date a lonely housewife always wanted to date ...</td>\n",
       "      <td>phish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52362</th>\n",
       "      <td>18646</td>\n",
       "      <td>18647</td>\n",
       "      <td>request submitted : access request for anita ....</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52363</th>\n",
       "      <td>18647</td>\n",
       "      <td>18648</td>\n",
       "      <td>re : important - prc mtg hi dorn &amp; john , as y...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52364</th>\n",
       "      <td>18648</td>\n",
       "      <td>18649</td>\n",
       "      <td>press clippings - letter on californian utilit...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51765 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0  \\\n",
       "1                 1           1   \n",
       "2                 2           2   \n",
       "3                 3           3   \n",
       "4                 4           4   \n",
       "5                 5           5   \n",
       "...             ...         ...   \n",
       "52360         18644       18645   \n",
       "52361         18645       18646   \n",
       "52362         18646       18647   \n",
       "52363         18647       18648   \n",
       "52364         18648       18649   \n",
       "\n",
       "                                              Email Text Email Type  \n",
       "1      gary , production from the high island larger ...        ham  \n",
       "2                 - calpine daily gas nomination 1 . doc        ham  \n",
       "3      fyi - see note below - already done .\\nstella\\...        ham  \n",
       "4      fyi .\\n- - - - - - - - - - - - - - - - - - - -...        ham  \n",
       "5      jackie ,\\nsince the inlet to 3 river plant is ...        ham  \n",
       "...                                                  ...        ...  \n",
       "52360  \\nRick Moen  a Ã©crit:> > I'm confused. I thou...        ham  \n",
       "52361  date a lonely housewife always wanted to date ...      phish  \n",
       "52362  request submitted : access request for anita ....        ham  \n",
       "52363  re : important - prc mtg hi dorn & john , as y...        ham  \n",
       "52364  press clippings - letter on californian utilit...        ham  \n",
       "\n",
       "[51765 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailDataset.dropna(inplace=True)\n",
    "\n",
    "empty_rows = emailDataset[emailDataset[\"Email Text\"] == \"empty\"]\n",
    "emailDataset.drop(empty_rows.index, inplace=True)\n",
    "\n",
    "emailDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ad3c0",
   "metadata": {},
   "source": [
    "## 7. Bulk Clean Dataset & Export to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5775e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/prokope/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/prokope/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/prokope/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/prokope/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK downloads\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6718ee3",
   "metadata": {},
   "source": [
    "Clean data by removing stop words, punctuation, special characters, and tokenize/lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8ab4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+|www\\.\\S+\", \" \", text)\n",
    "\n",
    "    # 3. Remove email addresses\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "\n",
    "    # 4. Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # 5. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 6. Remove stop words + non-alpha words\n",
    "    tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "\n",
    "    # 7. Lemmatize (optional but improves SVM performance)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f881c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emailDataset[\"clean_text\"] = emailDataset[\"Email Text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec7b69",
   "metadata": {},
   "source": [
    "Export to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eaa65c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Email Text</th>\n",
       "      <th>Email Type</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>gary , production from the high island larger ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>gary production high island larger block comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>- calpine daily gas nomination 1 . doc</td>\n",
       "      <td>ham</td>\n",
       "      <td>calpine daily gas nomination doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
       "      <td>ham</td>\n",
       "      <td>fyi see note already done stella forwarded ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
       "      <td>ham</td>\n",
       "      <td>fyi forwarded lauri allen hou ect pm kimberly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>jackie ,\\nsince the inlet to 3 river plant is ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>jackie since inlet river plant shut last day f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "5             5           5   \n",
       "\n",
       "                                          Email Text Email Type  \\\n",
       "1  gary , production from the high island larger ...        ham   \n",
       "2             - calpine daily gas nomination 1 . doc        ham   \n",
       "3  fyi - see note below - already done .\\nstella\\...        ham   \n",
       "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...        ham   \n",
       "5  jackie ,\\nsince the inlet to 3 river plant is ...        ham   \n",
       "\n",
       "                                          clean_text  \n",
       "1  gary production high island larger block comme...  \n",
       "2                   calpine daily gas nomination doc  \n",
       "3  fyi see note already done stella forwarded ste...  \n",
       "4  fyi forwarded lauri allen hou ect pm kimberly ...  \n",
       "5  jackie since inlet river plant shut last day f...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailDataset.to_csv(\n",
    "    \"data/email_dataset.csv\",\n",
    "    index=False,\n",
    ")\n",
    "emailDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0226497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a DataFrame\n",
    "# summary_df = emailDataset.copy()\n",
    "\n",
    "# # Text length metrics\n",
    "# summary_df[\"char_count\"] = summary_df[\"Email Text\"].str.len()\n",
    "# summary_df[\"word_count\"] = summary_df[\"Email Text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# # Count URLs\n",
    "# url_pattern = r\"http[s]?://\\S+|www\\.\\S+\"\n",
    "# summary_df[\"url_count\"] = summary_df[\"Email Text\"].apply(lambda x: len(re.findall(url_pattern, x)))\n",
    "\n",
    "# # Count special characters\n",
    "# summary_df[\"special_chars\"] = summary_df[\"Email Text\"].apply(lambda x: sum(not c.isalnum() and not c.isspace() for c in x))\n",
    "\n",
    "# # Count uppercase words\n",
    "# summary_df[\"uppercase_words\"] = summary_df[\"Email Text\"].apply(lambda x: len([w for w in x.split() if w.isupper()]))\n",
    "\n",
    "# # Save the DataFrame to a JSON file\n",
    "# summary_df[[\n",
    "#     \"char_count\",\n",
    "#     \"word_count\",\n",
    "#     \"url_count\",\n",
    "#     \"special_chars\",\n",
    "#     \"uppercase_words\",\n",
    "# ]].to_json('./data/email_dataset_summary_stats.json', orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
